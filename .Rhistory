cl = dta.training$location, k = best.k.features.relieff)
knn.model.mdl <- knn(train = dta.training[, mdl.subset], test  = dta.testing[, mdl.subset],
cl = dta.training$location, k = best.k.features.mdl)
# kNN: 'predict'
knn.preds.all <- knn.model.all
knn.preds.relieff <- knn.model.relieff
knn.preds.mdl <- knn.model.mdl
# ANN: train
ann.model.all <- nnet(formula =  as.formula(paste("location~",paste(full.features,collapse="+"))),
data = dta.training,
size = best.size.features.all,
trace = FALSE)
ann.model.relieff <- nnet(formula =  as.formula(paste("location~",paste(relieff.subset,collapse="+"))),
data = dta.training,
size = best.size.features.relieff,
trace = FALSE)
ann.model.mdl <- nnet(formula =  as.formula(paste("location~",paste(mdl.subset,collapse="+"))),
data = dta.training,
size = best.size.features.mdl,
trace = FALSE)
# ANN: predict
ann.preds.all <- predict(ann.model.all, dta.testing[,-9], type="class")
ann.preds.relieff <- predict(ann.model.relieff, dta.testing[,-9], type="class")
ann.preds.mdl <- predict(ann.model.mdl, dta.testing[,-9], type="class")
CalculatePerformance(dta.testing, simple.tree.preds)
CalculatePerformance(dta.testing, pruned.tree.preds)
CalculatePerformance(dta.testing, knn.model.all)
CalculatePerformance(dta.testing, knn.model.relieff)
CalculatePerformance(dta.testing, knn.model.mdl)
CalculatePerformance(dta.testing, ann.preds.all)
CalculatePerformance(dta.testing, ann.preds.relieff)
CalculatePerformance(dta.testing, ann.preds.mdl)
library("FSelector")
library("CORElearn")
library("tree") # Decision trees, function "tree"
library("class") # k Nearest Neighbors, function "knn"
library("nnet") # artificial neural network, function "nnet"
# library("C50")   # C5.0 decission tree, function "C5.0"
# Load utility libraries
source("dta.R")
source("util.R")
# Load our learning algorithms wrapped with kfold cv
source("kfold-knn.R")
source("kfold-ann.R")
#source("kfold-bayes.R")
## Load the data
dta.list <- LoadData(scale=T, discretize=F, randomize=F)
dta.training <- dta.list[[1]]
# dta.testing <- dta.list[[2]] # Comment since we don't touch this till we are finished
all.classes <- levels(dta.training$location)
full.features <- colnames(dta.training)[-9]
?tree
tree.model.simple <- tree(location~., dta.training)
summary(tree.model.simple)
plot(tree.model.simple, type="u")
text(tree.model.simple)
tree.model.cv <- cv.tree(tree.model.simple, FUN=prune.tree, K=5)
plot(tree.model.cv)
best.tree.size <- tree.model.cv$size[which.min(tree.model.cv$dev)]
cat(sprintf("Lowest dev (%g) at tree size %g \n",
min(tree.model.cv$dev),
best.tree.size))
tree.model.pruned <- prune.tree(tree.model.simple,best=best.tree.size)
summary(tree.model.pruned)
plot(tree.model.pruned, type="u")
text(tree.model.pruned)
## Tune KNN and naiveBayes with full feature set
features <- full.features
results.knn <- run_knn(dta.training, features, ks=c(1:30))
best.k.features.all <- which.max(results.knn$avg_auc)
# Run k-fold cv ANN to select best Size values
results.ann <- run_ann(dta.training, features, sizes=c(1:25))
best.size.features.all <- which.max(results.ann$avg_auc)
features.by.tree <- length(summary(tree.model.pruned)$used)
best.size.features.all <- which.max(results.ann$avg_auc)
features.by.tree <- length(summary(tree.model.pruned)$used)
relieff.weights <- relief(location ~ ., data = dta.training)
relieff.subset <- cutoff.biggest.diff(relieff.weights) # alternative
relieff.subset
relieff.subset <- cutoff.k(relieff.weights, features.by.tree)
relieff.subset
mdl.weights <- attrEval(location ~., dta.training, estimator="MDL")
mdl.subset <- names(sort(mdl.weights, decreasing=TRUE)[1:features.by.tree])
mdl.subset
?cutoff.biggest.diff
relieff.subset <- cutoff.biggest.diff(relieff.weights) # alternative
relieff.subset
mdl.weights <- attrEval(location ~., dta.training, estimator="MDL")
mdl.subset <- names(sort(mdl.weights, decreasing=TRUE)[1:features.by.tree])
mdl.subset
results.knn <- run_knn(dta.training, relieff.subset, ks=c(1:30))
best.k.features.relieff <- which.max(results.knn$avg_auc)
results.ann <- run_ann(dta.training, relieff.subset, sizes=c(1:20))
best.size.features.relieff <- which.max(results.ann$avg_auc)
results.knn <- run_knn(dta.training, mdl.subset, ks=c(1:30))
best.k.features.mdl <- which.max(results.knn$avg_auc)
results.ann <- run_ann(dta.training, mdl.subset, sizes=c(1:20))
best.k.features.mdl <- which.max(results.knn$avg_auc)
best.size.features.mdl <- which.max(results.ann$avg_auc)
## Train and test on the split data set (held out till paper-writing-day)
dta.testing <- dta.list[[2]] # Comment since we don't touch this till we are finished
simple.tree.preds <- predict(tree.model.simple, dta.testing[,-9], type="class")
pruned.tree.preds <- predict(tree.model.pruned, dta.testing[,-9], type="class")
knn.model.all <- knn(train = dta.training[ ,full.features], test  = dta.testing[, full.features],
cl = dta.training$location,  k = best.k.features.all)
knn.model.relieff <- knn(train = dta.training[, relieff.subset], test  = dta.testing[, relieff.subset],
cl = dta.training$location, k = best.k.features.relieff)
knn.model.mdl <- knn(train = dta.training[, mdl.subset], test  = dta.testing[, mdl.subset],
cl = dta.training$location, k = best.k.features.mdl)
# kNN: 'predict'
knn.preds.all <- knn.model.all
knn.preds.relieff <- knn.model.relieff
knn.preds.mdl <- knn.model.mdl
# ANN: train
ann.model.all <- nnet(formula =  as.formula(paste("location~",paste(full.features,collapse="+"))),
data = dta.training,
size = best.size.features.all,
trace = FALSE)
ann.model.relieff <- nnet(formula =  as.formula(paste("location~",paste(relieff.subset,collapse="+"))),
data = dta.training,
size = best.size.features.relieff,
trace = FALSE)
ann.model.mdl <- nnet(formula =  as.formula(paste("location~",paste(mdl.subset,collapse="+"))),
data = dta.training,
size = best.size.features.mdl,
trace = FALSE)
# ANN: predict
ann.preds.all <- predict(ann.model.all, dta.testing[,-9], type="class")
ann.preds.relieff <- predict(ann.model.relieff, dta.testing[,-9], type="class")
ann.preds.mdl <- predict(ann.model.mdl, dta.testing[,-9], type="class")
# All outputs
CalculatePerformance(dta.testing, simple.tree.preds)
CalculatePerformance(dta.testing, pruned.tree.preds)
CalculatePerformance(dta.testing, knn.model.all)
CalculatePerformance(dta.testing, knn.model.relieff)
CalculatePerformance(dta.testing, knn.model.mdl)
CalculatePerformance(dta.testing, ann.preds.all)
CalculatePerformance(dta.testing, ann.preds.relieff)
CalculatePerformance(dta.testing, ann.preds.mdl)
?C5.0Control
??C5.0Control
# We want to try out C5.0 as well
c5tree.model <- C5.0(x = dta.training[, full.features],
y = dta.training$location,
rules = F,
control = C5.0Control(
subset = T,
winnow = F,
earlyStopping = T,
))
library("C50")   # C5.0 decission tree, function "C5.0"
c5tree.model <- C5.0(x = dta.training[, full.features],
y = dta.training$location,
rules = F,
control = C5.0Control(
subset = T,
winnow = F,
earlyStopping = T,
))
library("FSelector")
library("CORElearn")
library("tree") # Decision trees, function "tree"
library("class") # k Nearest Neighbors, function "knn"
library("nnet") # artificial neural network, function "nnet"
library("C50")   # C5.0 decission tree, function "C5.0"
# Load utility libraries
source("dta.R")
source("util.R")
# Load our learning algorithms wrapped with kfold cv
source("kfold-knn.R")
source("kfold-ann.R")
#source("kfold-bayes.R")
dta.list <- LoadData(scale=T, discretize=F, randomize=F)
dta.training <- dta.list[[1]]
all.classes <- levels(dta.training$location)
full.features <- colnames(dta.training)[-9]
# We want to try out C5.0 as well
c5tree.model <- C5.0(x = dta.training[, full.features],
y = dta.training$location,
rules = F,
control = C5.0Control(
subset = T,
winnow = F,
earlyStopping = T,
))
c5tree.preds <- predict(c5tree.model, dta.testing[,-9], type="class")
dta.testing <- dta.list[[2]] # Comment since we don't touch this till we are finished
c5tree.preds <- predict(c5tree.model, dta.testing[,-9], type="class")
CalculatePerformance(dta.testing, c5tree.preds)
library("FSelector")
library("CORElearn")
library("tree") # Decision trees, function "tree"
library("class") # k Nearest Neighbors, function "knn"
library("nnet") # artificial neural network, function "nnet"
library("C50")   # C5.0 decission tree, function "C5.0"
# Load utility libraries
source("dta.R")
source("util.R")
# Load our learning algorithms wrapped with kfold cv
source("kfold-knn.R")
source("kfold-ann.R")
#source("kfold-bayes.R")
tree.model.simple <- tree(location~., dta.training)
# Folder to save plots
plots.dir <- "/Users/tjs/Desktop/results"
## Load the data
dta.list <- LoadData(scale=T, discretize=F, randomize=F)
dta.training <- dta.list[[1]]
# dta.testing <- dta.list[[2]] # Comment since we don't touch this till we are finished
all.classes <- levels(dta.training$location)
full.features <- colnames(dta.training)[-9]
tree.model.simple <- tree(location~., dta.training)
summary(tree.model.simple)
plot(tree.model.simple, type="u")
text(tree.model.simple)
dev.copy2pdf(file = paste0(plots.dir, "simple tree.pdf"))
paste0(plots.dir, "simple tree.pdf")
dev.copy2pdf(file = paste0(plots.dir, "simple tree.pdf"))
paste0(plots.dir, "simple tree.pdf")
plots.dir <- "/Users/tjs/Desktop/results/"
dev.copy2pdf(file = paste0(plots.dir, "simple tree.pdf"))
library("FSelector")
library("CORElearn")
library("tree") # Decision trees, function "tree"
library("class") # k Nearest Neighbors, function "knn"
library("nnet") # artificial neural network, function "nnet"
library("C50")   # C5.0 decission tree, function "C5.0"
# Load utility libraries
source("dta.R")
source("util.R")
# Load our learning algorithms wrapped with kfold cv
source("kfold-knn.R")
source("kfold-ann.R")
#source("kfold-bayes.R")
plots.dir <- "/Users/tjs/Desktop/results/" # make sure to end with a slash
## Load the data
dta.list <- LoadData(scale=T, discretize=F, randomize=F)
dta.training <- dta.list[[1]]
# dta.testing <- dta.list[[2]] # Comment since we don't touch this till we are finished
all.classes <- levels(dta.training$location)
full.features <- colnames(dta.training)[-9]
## Try and evaluate the Decision Tree (using full featureset)
# Build a simple tree model
tree.model.simple <- tree(location~., dta.training)
summary(tree.model.simple)
plot(tree.model.simple, type="u")
text(tree.model.simple)
dev.copy2pdf(file = paste0(plots.dir, "simple tree.pdf"))
?dev.copy2pdf
dev.copy2pdf(file = paste0(plots.dir, "simple tree.pdf"), displaylist="inhibit")
dev.copy2pdf(file = paste0(plots.dir, "simple tree.pdf"), displaylist=c("inhibit"))
dev.copy2pdf(file = paste0(plots.dir, "simple tree.pdf"))
tree.model.complex <- tree(location~., dta.training, control=tree.control(nobs=nrow(dta.training),
mincut=2, minsize=5, mindev=0.0025))
tree.model.complex.cv <- cv.tree(tree.model.complex, FUN=prune.tree, K=5)
summary(tree.model.complex)
tree.model.complex.cv <- cv.tree(tree.model.complex, FUN=prune.tree, K=5)
plot(tree.model.complex.cv)
dev.copy2pdf(file = paste0(plots.dir, "complex tree cv.pdf"))
best.tree.size <- tree.model.complex.cv$size[which.min(tree.model.complex.cv$dev)]
cat(sprintf("Lowest dev (%g) at tree size %g \n",
min(tree.model.complex.cv$dev),
best.tree.size))
tree.model.pruned <- prune.tree(tree.model.complex,best=best.tree.size)
summary(tree.model.pruned)
plot(tree.model.pruned, type="u")
text(tree.model.pruned)
dev.copy2pdf(file = paste0(plots.dir, "pruned complex tree.pdf"))
# We want to try out C5.0 as well
c5tree.model <- C5.0(x = dta.training[, full.features],
y = dta.training$location,
rules = F,
control = C5.0Control(
subset = T,
winnow = F,
earlyStopping = T,
))
## Tune KNN and naiveBayes with full feature set
c5tree.model <- C5.0(x = dta.training[, full.features], y = dta.training$location,
rules = F, control = C5.0Control(subset = T, winnow = F, earlyStopping = T))
## Tune KNN and naiveBayes with full feature set
# Run k-fold cv KNN to select best K values
results.knn <- run_knn(dta.training, full.features, ks=c(1:30))
dev.copy2pdf(file = paste0(plots.dir, "knn-full.pdf"))
best.k.features.all <- which.max(results.knn$avg_auc)
# Run k-fold cv ANN to select best Size values
results.ann <- run_ann(dta.training, full.features, sizes=c(1:20))
dev.copy2pdf(file = paste0(plots.dir, "ann-full.pdf"))
best.size.features.all <- which.max(results.ann$avg_auc)
best.size.features.all <- which.max(results.ann$avg_auc)
best.k.features.all <- which.max(results.knn$avg_auc)
features.by.tree <- length(summary(tree.model.simple)$used)
relieff.weights <- relief(location ~ ., data = dta.training)
relieff.subset <- cutoff.biggest.diff(relieff.weights) # alternative
relieff.subset
mdl.weights <- attrEval(location ~., dta.training, estimator="MDL")
mdl.subset <- names(sort(mdl.weights, decreasing=TRUE)[1:features.by.tree])
mdl.subset
relieff.subset <- cutoff.biggest.diff(relieff.weights)
relieff.subset
relieff.subset <- cutoff.k(relieff.weights, features.by.tree)
relieff.subset
relieff.subset
mdl.subset
results.knn <- run_knn(dta.training, relieff.subset, ks=c(1:30))
dev.copy2pdf(file = paste0(plots.dir, "knn-relieff.pdf"))
best.k.features.relieff <- which.max(results.knn$avg_auc)
# Run k-fold cv ANN to select best Size values
results.ann <- run_ann(dta.training, relieff.subset, sizes=c(1:20))
dev.copy2pdf(file = paste0(plots.dir, "ann-relieff.pdf"))
best.size.features.relieff <- which.max(results.ann$avg_auc)
results.knn <- run_knn(dta.training, mdl.subset, ks=c(1:30))
dev.copy2pdf(file = paste0(plots.dir, "knn-mdl.pdf"))
best.k.features.mdl <- which.max(results.knn$avg_auc)
# Run k-fold cv ANN to select best Size values
results.ann <- run_ann(dta.training, mdl.subset, sizes=c(1:20))
dev.copy2pdf(file = paste0(plots.dir, "ann-mdl.pdf"))
best.size.features.mdl <- which.max(results.ann$avg_auc)
dta.testing <- dta.list[[2]] # Comment since we don't touch this till we are finished
simple.tree.preds <- predict(tree.model.simple, dta.testing[,-9], type="class")
pruned.tree.preds <- predict(tree.model.pruned, dta.testing[,-9], type="class")
c5tree.preds <- predict(c5tree.model, dta.testing[,-9], type="class")
# kNN: train
knn.model.all <- knn(train = dta.training[ ,full.features], test  = dta.testing[, full.features],
cl = dta.training$location,  k = best.k.features.all)
knn.model.relieff <- knn(train = dta.training[, relieff.subset], test  = dta.testing[, relieff.subset],
cl = dta.training$location, k = best.k.features.relieff)
knn.model.mdl <- knn(train = dta.training[, mdl.subset], test  = dta.testing[, mdl.subset],
cl = dta.training$location, k = best.k.features.mdl)
# kNN: 'predict'
knn.preds.all <- knn.model.all
knn.preds.relieff <- knn.model.relieff
knn.preds.mdl <- knn.model.mdl
# ANN: train
ann.model.all <- nnet(formula =  as.formula(paste("location~",paste(full.features,collapse="+"))),
data = dta.training,
size = best.size.features.all,
trace = FALSE)
ann.model.relieff <- nnet(formula =  as.formula(paste("location~",paste(relieff.subset,collapse="+"))),
data = dta.training,
size = best.size.features.relieff,
trace = FALSE)
ann.model.mdl <- nnet(formula =  as.formula(paste("location~",paste(mdl.subset,collapse="+"))),
data = dta.training,
size = best.size.features.mdl,
trace = FALSE)
# ANN: predict
ann.preds.all <- predict(ann.model.all, dta.testing[,-9], type="class")
ann.preds.relieff <- predict(ann.model.relieff, dta.testing[,-9], type="class")
ann.preds.mdl <- predict(ann.model.mdl, dta.testing[,-9], type="class")
# All outputs
CalculatePerformance(dta.testing, simple.tree.preds)
CalculatePerformance(dta.testing, pruned.tree.preds)
CalculatePerformance(dta.testing, c5tree.preds)
CalculatePerformance(dta.testing, knn.model.all)
CalculatePerformance(dta.testing, knn.model.relieff)
CalculatePerformance(dta.testing, knn.model.mdl)
CalculatePerformance(dta.testing, ann.preds.all)
CalculatePerformance(dta.testing, ann.preds.relieff)
CalculatePerformance(dta.testing, ann.preds.mdl)
save.image("~/Development/MLProject/paper-run2.RData")
summary(tree.model.simple)$used
relieff.subset
mdl.subset
summary(tree.model.pruned)
summary(tree.model.pruned)
plot(tree.model.pruned, type="u")
text(tree.model.pruned)
pruned.tree.preds <- predict(tree.model.pruned, dta.testing[,-9], type="class")
CalculatePerformance(dta.testing, pruned.tree.preds)
load("~/Development/MLProject/paper-run2.RData")
summary(tree.model.pruned)
plot(tree.model.pruned, type="u")
text(tree.model.pruned)
tree.model.pruned
?prune.tree
??prune.tree
tree.model.complex.cv <- cv.tree(tree.model.complex, FUN=prune.tree, K=5)
plot(tree.model.complex.cv)
library("FSelector")
library("CORElearn")
library("tree") # Decision trees, function "tree"
library("class") # k Nearest Neighbors, function "knn"
library("nnet") # artificial neural network, function "nnet"
library("C50")   # C5.0 decission tree, function "C5.0"
# Load utility libraries
source("dta.R")
source("util.R")
# Load our learning algorithms wrapped with kfold cv
source("kfold-knn.R")
source("kfold-ann.R")
#source("kfold-bayes.R")
summary(tree.model.pruned)
?cv.tree
?prune.tree
tree.model.complex.cv <- cv.tree(tree.model.complex, FUN=prune.misclass, K=5)
plot(tree.model.complex.cv)
best.tree.size <- tree.model.complex.cv$size[which.min(tree.model.complex.cv$dev)]
tree.model.pruned <- prune.misclass(tree.model.complex,best=best.tree.size)
summary(tree.model.pruned)
plot(tree.model.pruned, type="u")
tree.model.complex.cv <- cv.tree(tree.model.complex, FUN=prune.misclass, K=5)
plot(tree.model.complex.cv)
tree.model.complex.cv <- cv.tree(tree.model.complex, FUN=prune.misclass, K=5)
plot(tree.model.complex.cv)
# dev.copy2pdf(file = paste0(plots.dir, "complex tree cv.pdf"))
best.tree.size <- tree.model.complex.cv$size[which.min(tree.model.complex.cv$dev)]
cat(sprintf("Lowest dev (%g) at tree size %g \n",
min(tree.model.complex.cv$dev),
best.tree.size))
tree.model.pruned <- prune.misclass(tree.model.complex,best=best.tree.size)
summary(tree.model.pruned)
load("~/Development/MLProject/paper-run2.RData")
tree.model.pruned <- prune.tree(tree.model.complex,best=best.tree.size)
summary(tree.model.pruned)
library("FSelector")
library("CORElearn")
library("tree") # Decision trees, function "tree"
library("class") # k Nearest Neighbors, function "knn"
library("nnet") # artificial neural network, function "nnet"
library("C50")   # C5.0 decission tree, function "C5.0"
# Load utility libraries
source("dta.R")
source("util.R")
# Load our learning algorithms wrapped with kfold cv
source("kfold-knn.R")
source("kfold-ann.R")
#source("kfold-bayes.R")
summary(tree.model.pruned)
?tree.control
cite(FSelector)
library("nnet") # artificial neural network, function "nnet"
?nnet
CORElearn
library("CORElearn")
cite(CORELearn)
cite("CORELearn")
cite("FSelector")
cite("nnet")
cite("pROC")
?cite
View(dta.testing)
source('~/.active-rstudio-document')
apply(location ~ ., dta.testing)
formula = location ~ .
strsplit(format(formula), ' ')
column = strsplit(format(formula), ' ')
column
column[0]
column[1]
column[1][1]
column[[1]]
unique(rapply(column, function(x) head(x, 1)))
unique(rapply(strsplit(format(formula), ' '), function(x) head(x, 1)))
dta.testing[, "location"]
column = strsplit(format(formula), ' ')
column
head(column)
head(column, 1)
d = factor("sexy", "lady")
d*d
mutinfo
as.numeric(d)
as.character(d)
as.factor(c(3,3,3,3,2,2,2,3,1,NA,NA,2,2))
package fnn
install.packages("FNN")
library("FNN", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
mutinfo
is_numeric(d)
is.numeric(d)
mutinfo(d,d)
mutinfo(d,d) == 0
mutinformation
install.packages("infotheo")
mutinformation
library("infotheo", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
mutinformation
entropy(d)
d
entropy(factor(1,2,3,1))
d = as.factor(c(4,2,3,4,3,4,4,4,3,4,3,3,3))
d
entropy(d)
entropy(as.factor(c(3,1,4,1,5)))
entropy(factor("foo", "bar"))
as.numeric("foo")
factor
> entropy(factor("foo", "bar"))
[1] NaN
d = as.factor(c(3,1,4,1,5)))
levels(d)
d = factor("sexy", "lady")
levels(d)
d
d = as.factor(c("foo", "bar"))
d
entropy(d)
?factor
factor(c("foo", "bar"))
mutinformation( factor(c("foo", "bar")),  factor(c("foo", "bar")))
mutinformation( factor(c(3,1,4,1,5)),  factor(c(3,1,4,1,5)))
mutinformation( c(3,1,4,1,5),  c(3,1,4,1,5))
library("nnet")
nnet?
?
;
?nnnet
??nnet
library("nnet")
??nnet
?nnet
