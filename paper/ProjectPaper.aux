\relax 
\citation{yang200610}
\citation{kotsiantis2006handling}
\citation{wang2012multiclass}
\citation{chawla2011smote}
\citation{japkowicz2002class}
\citation{ertekin2007learning,hospedales2013finding}
\citation{mitchell1997machine}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Yeast data}{1}}
\citation{quinlan1993c4}
\citation{russell13artificial}
\citation{aha1991instance}
\citation{mitchell1997machine}
\citation{dash1997feature}
\citation{kononenko1994estimating}
\citation{kira1992feature}
\citation{dash1997feature,robnik2003theoretical}
\citation{dash1997feature}
\citation{sheinvald1990modeling}
\citation{rissanen1978modeling}
\citation{dash1997feature}
\citation{procR}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Learning algorithms}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Decision trees}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Instance based learning}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Artificial Neural Network (ANN)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Feature selection}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Relief-F}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Minimum Description Length Method (MDLM)}{2}}
\citation{sokolova2006beyond}
\citation{fawcett2006introduction}
\citation{hand2001simple}
\citation{procR}
\@writefile{toc}{\contentsline {section}{\numberline {2}Experiments}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Performance measure}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Decision tree}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Decision tree (C4.5) obtained using the default settings. There are 9 terminal nodes that represent 7 out of 10 class labels. The features selected by the learning algorithm are \{alm, gvh, mcg, mit, nuc\}.\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{default}{{1}{3}}
\newlabel{fig:sub1}{{2a}{4}}
\newlabel{sub@fig:sub1}{{a}{4}}
\newlabel{fig:sub2}{{2b}{4}}
\newlabel{sub@fig:sub2}{{b}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results from cross-validation to prune the complex tree (a), and the resulting pruned tree (b).\relax }}{4}}
\newlabel{fig:test}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\textit  {k}-NN and ANN with full featureset}{4}}
\newlabel{fig:sub1}{{3a}{4}}
\newlabel{sub@fig:sub1}{{a}{4}}
\newlabel{fig:sub2}{{3b}{4}}
\newlabel{sub@fig:sub2}{{b}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results from 5-fold cross-validation to select the optimal value for \textit  {k} in our \textit  {k}-NN algorithm (a), and optimal number of units in hidden layer for ANN algorithm (b), using the full feature set.\relax }}{4}}
\newlabel{fig:test}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}\textit  {k}-NN and ANN with Relief-F features}{4}}
\newlabel{fig:sub1}{{4a}{5}}
\newlabel{sub@fig:sub1}{{a}{5}}
\newlabel{fig:sub2}{{4b}{5}}
\newlabel{sub@fig:sub2}{{b}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Results from 5-fold cross-validation to select the optimal value for \textit  {k} in our \textit  {k}-NN algorithm (a), and optimal number of units in hidden layer for ANN algorithm (b), using the feature set obtained through the Relief-F feature selection algorithm.\relax }}{5}}
\newlabel{fig:test}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}\textit  {k}-NN and ANN with MDL features}{5}}
\newlabel{fig:sub1}{{5a}{5}}
\newlabel{sub@fig:sub1}{{a}{5}}
\newlabel{fig:sub2}{{5b}{5}}
\newlabel{sub@fig:sub2}{{b}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Results from 5-fold cross-validation to select the optimal value for \textit  {k} in our \textit  {k}-NN algorithm (a), and optimal number of units in hidden layer for ANN algorithm (b), using the feature set obtained through the MDML feature selection algorithm.\relax }}{5}}
\newlabel{fig:test}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Results}{5}}
\bibdata{bib}
\bibcite{aha1991instance}{1}
\bibcite{chawla2011smote}{2}
\bibcite{dash1997feature}{3}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results obtained from testing the classifiers on the held-out testing set, after they were trained on the full training set.\relax }}{6}}
\newlabel{default}{{1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusions}{6}}
\bibcite{ertekin2007learning}{4}
\bibcite{fawcett2006introduction}{5}
\bibcite{hand2001simple}{6}
\bibcite{hospedales2013finding}{7}
\bibcite{japkowicz2002class}{8}
\bibcite{kira1992feature}{9}
\bibcite{kononenko1994estimating}{10}
\bibcite{kotsiantis2006handling}{11}
\bibcite{mitchell1997machine}{12}
\bibcite{quinlan1993c4}{13}
\bibcite{rissanen1978modeling}{14}
\bibcite{procR}{15}
\bibcite{robnik2003theoretical}{16}
\bibcite{russell13artificial}{17}
\bibcite{sheinvald1990modeling}{18}
\bibcite{sokolova2006beyond}{19}
\bibcite{wang2012multiclass}{20}
\bibcite{yang200610}{21}
\bibstyle{plain}
